# Summary of the Alternating Descent Conditional Gradient Method for Sparse Inverse Problems (Nicholas Boyd, Geoffrey Schiebinger and Benjamin Recht)

## From the abstract

* Scope: Sparse inverse problems with differentiable measurement models.

* Practical problems: Superresolution, time-series modelling and matrix completion.

* Key ingredients: nonconvex and convex optimization techniques. Global conditional gradient steps alternating with nonconvex local search exploiting the differentiable measurement model.

## From the introduction

* Common prior in statistical signal processing: observed signal is the noisy measurement of a few weighted sources.
Hence solving the sparse invese problem amounts to finding a collection of a few parameters and weights that adequately explains the observed signal.

* Claim: See the estimation problem as an optimization problem over the space of measures.

* See the conditional gradient method (CGM) also known as the Frank-Wolfe algorithm.

### Mathematical setup

* Hypothesis: we assume the existence of an underlying collection of sources. Each source has a non-negative weight $\omega > 0$ and a parameter $\theta \in \Theta$. (e.g. $\theta$ -> position / orientation / polarization, $\omega$ -> intensity / distance from the measurement device.

* Goal: Recover the number of sources present along with their individual weights and parameters.

* Scope: Sources are not observed directly but we are given a noisy measurement in $\mathbb{R}^d$.

* Hypothesis: Measurement model is completely determined by the function $\phi$ from $\Omega$ to $\mathbb{R}^d$. Degree one homogeneity of the sources measurement in its weight. Measurement generated by a single source is $\omega \phi(\omega) \in \mathbb{R}^d$. Measurement of a weighted collection of sources is **additive**. $\sum_{i=1}^{K} \omega_i \phi(\theta_i) \in \mathbb{R}^d$ is the noise-free measurement.

* Note: recovering the signal parameters of a corrupted measurement without any prior information is impossible in most interesting problems as the operator is almost never jective.

* Estimation: the signal parameters encoded in $\mu_{\text{true}}$ are estimated by minimzing a convex loss $l$ of the residual between $y$ and $\Phi \mu$:
$$\text{minimize} l(\Phi \mu - y) \text{s.t.} \mu \meq 0 \text{and} |\text{supp}(\mu)| \leq N$$

* Convexity analysis: $l$ is convex but the constraint on the support of $\mu$ is nonconvex. Surrogate for a cardinality constraint on a nonnegative measure is a constraint on the total mass ($\mu(\Theta) \leq \tau$).

## Example applications

* Superresolution imaging: remove the blur induced by diffraction as well as the effects of pixelization and noise. SOTA: localizing point sources in a fluorescence microscopy challenge dataset.

* Linear system identification: such a system describes the evolution of an output $y_t \in \mathbb{R}$ based on the input $u_t \in \mathbb{R}$, where $t \in \mathbb{Z}_\plus$ indexes time. The internal state at time $t$ of the system is parameterized by a vector $x_t \in \mathbb{R}^m$ and its relationship to the output is described by:
$$x_{t+1} = A x_t + B u_t$$
$$y_t = C x_t$$
$C$ is a fixed matrix, while $x_0$, $A$, and $B$ are unknown parameters. SOTA: two standard system identification datasets.

* Matrix completion: the task of matrix completion is to estimate all entries of a large matrix given observations of a few entries. Prior: low-rank matrix. SOTA: Netflix challenge.

* Bayesian experimental design. Experimental design: estimate a vector $x \in \mathbb{R}^d$ from measurements of the form:
$$y_i = f(\theta_i)^T x + \epsilon_i$$
Where $f$ from $\Theta$ to $\mathbb{R}^d$ is a known differentiable feature function and $\epsilon_i$ are independent noise terms. If each measurement requires a costly experiment, this corresponds to getting the most information from a fixed number of experiments. Assuming $\epsilon_i$ drawn from i.i.d. standard normals and $x$ coming from a standard normal prior, the posterior distribution of $x$ given $y_1, \hdot, y_k$ can be analytically derived since the full joint distribution of $x, y_1, \hdot, y_m$ is normal.$\theta_i, \hdot, \theta_k$ are chose to minimize the entropy of the posterior, which is equivalent to minimizing the (log) volume of an uncertainty ellipsoid.

* Fitting misture models ot data. Recover the mixing distribution $\pi$ given the data$\{x_1, \hdot, x_d\}$.

* Design of numerical quadrature rules. Numerical computing applications require fast procedures to approximate integration against a fixed measure. Recover the quadrature nodes.

* Neural spike identification. Recover the distance between $i$th neuron and the electrode ($w_i > 0$) and the parameters of the $i$th neuron ($\theta_i = (t_i, k_i)$) from the voltage signal $v$. 

* Designing radiation therapy. Fire several beams of radiation at the patient to irradiate tumors. The collection of beam parameters (intensities, positions and angles) is called the treatment plan. Objective function usually rewards giving large doses of radiation to tumors and low dosages to surrounding healthy tissue and vital organs. Plans with few beams are required.

## Conditional gradient method

CGM optimization problem: $$\text{minimize} f(x) {s.t.} x \in \mathcal{C}$$ where $\mathcal{C}$ is a closed, bounded ,and convex set and $f$ is a differentiable convex function.

* Steps in main loop:
  * Linearize
  * Minimize the linearized function
  * Tentative update
  * Final update

The minimization step in (2) may be difficult and requires an application-specific subroutine.

A common update scheme for step (4) is to do a line-search in the convex set induced by $(x_k, s_k)$ or $(x_k, s_1, \hdot, s_k)$.

Note that it is very simple to compute a lower bound on the optimal value as the algorithm runs by convexity of f.

### CGM for sparse inverse problems
Challenge: a general measure cannot be represented on a computer unless it is finitely-supported. Here we operate ov er the space of measures and denote $\mu_k$ instead of $x_k$

Linearization: Using the space of measures, in the finite dimensional case, we minimize the linarized objective over the constraint set in the second step of the CGM.

The optimal solution of the optimization problem with interchanged integral is the point-mass $\tau \delta_{\theta_{*}}$ where $\theta_^{*} \in \text{arg min} F(\theta)$ unless $F(\theta)$ is positive everywhere in which case the optimal solution is the 0 measure. This means that at each step of the CGM we only need to add a single point to the support of our approximate solution $\mu_k$ (Recall that $F(\theta) = \grad l(r_k)^T \phi(\theta)$).

Note: in essence the algorithm alternates between selecting a source to add to the support, and tuning the weights to lower the current cost. The tuning step (4) is a finite-dimensional convex optimization problem that we can solve with an off-the-shelf algorithm.

* Conditional gradient method for measures (CGM-M) steps:
  1. Compute gradient of loss
  2. Compute next source
  3. Update support
  4. Compute weights
  5. Prune support

CGM-M is limited by the fact that it can only change the support of the measure by adding and removing points. It cannot smoothly move $S_k$ within $\Theta$. We need to allow the support to move continuously within $\Theta$ by exploiting the differentiability of $\phi$ to locally improve the support at each iteration.

* Alternating descent conditional gradient method (ADCG):
  1. Compute gradient of loss
  2. Compute next source
  3. Update support
  4. Coordinate descent on nonconvex objective:
    Repeat
    1. Compute weights
    2. Prune support
    3. Locally improve support (``local_descent``)

* ``local_descent`` is a subroutine that takes a measure $\mu_k$ with atomic representation and attempts to use gradient information to reduce the function $l(\sum_{i=1}^{m}{w_i \phi(\theta_i) - y})$ holding the weights fixed.

* When the number of sources is held fixed, the optimization problem 
$$\text{minimize} l(\sum_{i=1}^{k}w_i \phi(\theta_i) - y) \text{s.t.} w_i \meq 0, \theta_i \in \Theta \text{and} \sum_i w_i \leq \tau$$
is nonconvex.

* Step 4 is a block coordinate descent over w_i and \theta_i.

* The algorithm as a whole can be interpreted as alternating between performing descent on the convex problem (infinite-dimensional) in step (2) and descent over the finite-dimensional (nonconvex) problem in step (4).

* Adding the local search dramatically improves the performance of the CGM. Thanks to the local improvement step, ADCG works well even without exact minimzation in step 2.

#### Interface and implementation
* Running ADCG comes down to two subroutine operations to compute :
  * $\phi(\theta)$ and $\frac{\partial \phi(\theta)}{\partial \theta}$ for $\theta in \Theta$.
  * $\arg \min \phi(\theta)^T v$ for any arbitrary vectors $v \in \mathbb{R}^d$.
  
* The second step is application specific and can be approximated while still yielding good results with ADCG.

### Related work
* Why is conditional gradient method as a general purpose solver for constrained inverse problems better than projected or proximal gradient methods ?
*Because it only requires linear optimization over the constraint set rather than solving a quadratic optimization.*

* ADCG can be seen as an instance of CoGENT(?) specialized to the case of measures and differentiable measurement models. 

* Question: Examples of non differentiable measurement models ?

* ADCG more general than nuclear-norm problems.

* Approximately solving by gridding the parameter space and solving the resulting finite dimensional problem is often not tractable even for fairly modest dimension. It also requires post-processing to induce more sparsity. However this approach is still SOTA in neuroscience, superresolution fluorescence microscopy, radar, remote sensing, compressive sensing and polynomial interpolation.

### Theoretical guarantees

#### Bounded memory
We need to store at most $d+1$ points where $d$ is the dimension of the measurements. In practice this needs to be implemented.

#### Convergence analysis
Accuracy $\delta$ in $O(\frac{1}{\delta})$ iterations.

Not so clear as of now.

### Numerical results

SOTA in:
1. Superresolution fluorescence microscopy
2. Matrix completion
3. System identification

### Conclusion and future work
* Does not require post-processing
* We can understand ADCG as a method to rigorously control local search. There is a coupling between local search heuristicds and convex optimization.

* Tighten convergence analysis for ADCG: investigate if the bounds for ADCG can be tightened at all to be more predictive of practical performance (same as conditional gradient method).

* Relaxation to clustering algorithms: explore the connection between CGM and clustering algorithms like k-means. K-means is initialized by randomly seeking the points that are farthest from the current centers.

* Connections to cutting plane methods and semi-infinite programs

* Other applications
